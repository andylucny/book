{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "D5yFz-6HXVQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O \"twocats.jpg\" \"http://images.cocodataset.org/val2017/000000039769.jpg\""
      ],
      "metadata": {
        "id": "z4S78t5oPUFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open('twocats.jpg') # 640 x 480 x 3"
      ],
      "metadata": {
        "id": "h5UZL-9mPXzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6HjtuS1bT7N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the DETR model pretrained for the COCO dataset\n",
        "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
        "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")"
      ],
      "metadata": {
        "id": "HiEMhPllPoZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(images=image, return_tensors=\"pt\")\n",
        "# inputs 'pixel_values': 1 x 3 x 800 x 1066 'pixel_mask': 1 x 800 x 1066"
      ],
      "metadata": {
        "id": "QBCq4sBvQMrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in inputs.items():\n",
        "    print(f'{key}: {value.shape}')"
      ],
      "metadata": {
        "id": "hv0lts78Qc88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "outputs = model(**inputs) # -> 'logits', 'pred_boxes', 'last_hidden_state', 'encoder_last_hidden_state'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "S67Ec7rhQiQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for key, value in outputs.items():\n",
        "    print(f'{key}: {value.shape if hasattr(value, 'shape') else value}')\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5hQu_A5uQVad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backbone & pixel mask\n",
        "features, object_queries_list = model.model.backbone(pixel_values=inputs['pixel_values'], pixel_mask=inputs['pixel_mask'])\n",
        "feature_map, mask = features[-1]\n",
        "projected_feature_map = model.model.input_projection(feature_map)"
      ],
      "metadata": {
        "id": "nDS3ZdjXRSQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# sinusoidal position encoding (inside backbone)\n",
        "pixel_values=inputs['pixel_values']\n",
        "pixel_mask = inputs['pixel_mask']\n",
        "y_embed = pixel_mask.cumsum(1, dtype=torch.float32)\n",
        "x_embed = pixel_mask.cumsum(2, dtype=torch.float32)\n",
        "y_embed = y_embed / (y_embed[:, -1:, :] + 1e-6) * model.model.backbone.position_embedding.scale # 6.283185307179586\n",
        "x_embed = x_embed / (x_embed[:, :, -1:] + 1e-6) * model.model.backbone.position_embedding.scale # 6.283185307179586\n",
        "# model.model.backbone.position_embedding.embedding_dim 128\n",
        "dim_t = torch.arange(model.model.backbone.position_embedding.embedding_dim, dtype=torch.int64, device=pixel_values.device).float()\n",
        "# dim_t 0, 1, 2, ..., 127\n",
        "# model.model.backbone.position_embedding.temperature 10000\n",
        "dim_t = model.model.backbone.position_embedding.temperature ** (2 * torch.div(dim_t, 2, rounding_mode=\"floor\") / model.model.backbone.position_embedding.embedding_dim)\n",
        "pos_x = x_embed[:, :, :, None] / dim_t\n",
        "pos_y = y_embed[:, :, :, None] / dim_t\n",
        "pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
        "# pos == object_queries_list\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "B5dCY997Rn9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder\n",
        "flattened_features = projected_feature_map.flatten(2).permute(0, 2, 1)\n",
        "flattened_mask = mask.flatten(1)\n",
        "object_queries = object_queries_list[-1].flatten(2).permute(0, 2, 1)\n",
        "encoder_outputs =model.model.encoder(\n",
        "    inputs_embeds=flattened_features,\n",
        "    attention_mask=flattened_mask,\n",
        "    object_queries=object_queries,\n",
        "    output_attentions=None,\n",
        "    output_hidden_states=None,\n",
        "    return_dict=None,\n",
        ")\n",
        "hidden_states = encoder_outputs[0] # 1 x 850 x 256"
      ],
      "metadata": {
        "id": "wRKSjWscRxKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder\n",
        "\n",
        "# model.model.query_position_embeddings Embedding(100, 256)\n",
        "batch_size = hidden_states.shape[0]\n",
        "query_position_embeddings = model.model.query_position_embeddings.weight.unsqueeze(0).repeat(batch_size, 1, 1)\n",
        "# query_position_embeddings 1 x 100 x 256\n",
        "queries = torch.zeros_like(query_position_embeddings) # 1 x 100 x 256   zeros\n",
        "decoder_outputs = model.model.decoder(\n",
        "    inputs_embeds=queries,\n",
        "    attention_mask=None,\n",
        "    object_queries=object_queries,\n",
        "    query_position_embeddings=query_position_embeddings,\n",
        "    encoder_hidden_states=hidden_states,\n",
        "    encoder_attention_mask=flattened_mask,\n",
        "    output_attentions=None,\n",
        "    output_hidden_states=None,\n",
        "    return_dict=None,\n",
        ")\n",
        "sequence_output = decoder_outputs.last_hidden_state # 1 x 100 x 256"
      ],
      "metadata": {
        "id": "0UVqA8FTR--8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# post-processing\n",
        "\n",
        "# convert outputs (bounding boxes and class logits) to COCO API\n",
        "logits = model.class_labels_classifier(sequence_output) # 1 x 100 x 92\n",
        "boxes = model.bbox_predictor(sequence_output).sigmoid() # 1 x 100 x 4\n",
        "probabilities = torch.nn.functional.softmax(logits, -1) # 1 x 100 x 92\n",
        "scores, labels = probabilities[..., :-1].max(-1) # 1 x 100, 1 x 100\n",
        "\n",
        "# let's only keep detections with score > 0.9\n",
        "threshold = 0.9\n",
        "selection = scores > threshold\n",
        "scores = scores[selection]\n",
        "labels = labels[selection]\n",
        "boxes = boxes[selection]\n",
        "target_sizes = torch.tensor(image.size[::-1])\n",
        "\n",
        "def convert_boxes(x, width, height):\n",
        "    x_c, y_c, w, h = x.unbind(-1)\n",
        "    b = [(x_c - 0.5 * w)*width, (y_c - 0.5 * h)*height, (x_c + 0.5 * w)*width, (y_c + 0.5 * h)*height]\n",
        "    return torch.stack(b, dim=-1)\n",
        "\n",
        "# get boxes\n",
        "boxes = convert_boxes(boxes, target_sizes[1], target_sizes[0])\n",
        "print(boxes)"
      ],
      "metadata": {
        "id": "YgjZNPLMSIbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualization\n",
        "disp = cv2.cvtColor(np.array(image),cv2.COLOR_RGB2BGR)\n",
        "for score, label, box in zip(scores, labels, boxes):\n",
        "    box = [int(i) for i in box.tolist()]\n",
        "    cv2.rectangle(disp,box[:2],box[2:],(0,255,0),1)\n",
        "    label_text = model.config.id2label[label.item()]\n",
        "    cv2.putText(disp,label_text,(box[0]+4,box[1]+16),0,0.8,(0,255,0),1)\n",
        "\n",
        "cv2_imshow(disp)"
      ],
      "metadata": {
        "id": "r3WvSf2ESQIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "nMWmZAg4SaHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder with probes\n",
        "\n",
        "# model.model.query_position_embeddings Embedding(100, 256)\n",
        "batch_size = hidden_states.shape[0]\n",
        "query_position_embeddings = model.model.query_position_embeddings.weight.unsqueeze(0).repeat(batch_size, 1, 1)\n",
        "# query_position_embeddings 1 x 100 x 256\n",
        "queries = torch.zeros_like(query_position_embeddings) # 1 x 100 x 256   zeros\n",
        "\n",
        "sequence_outputs = []\n",
        "for i in range(6):\n",
        "    decoder_outputs = model.model.decoder(\n",
        "        inputs_embeds=queries,\n",
        "        attention_mask=None,\n",
        "        object_queries=object_queries,\n",
        "        query_position_embeddings=query_position_embeddings,\n",
        "        encoder_hidden_states=hidden_states,\n",
        "        encoder_attention_mask=flattened_mask,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    )\n",
        "    sequence_output = decoder_outputs.last_hidden_state # 1 x 100 x 256\n",
        "    sequence_outputs.append(sequence_output)\n",
        "    if i < 5:\n",
        "        del model.model.decoder.layers[-1]\n",
        "\n",
        "sequence_outputs.append(query_position_embeddings)\n",
        "sequence_outputs.append(hidden_states)\n",
        "sequence_outputs.reverse()"
      ],
      "metadata": {
        "id": "Tvg3kPGhSb9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the best detection indices\n",
        "sequence_output = sequence_outputs[-1]\n",
        "logits = model.class_labels_classifier(sequence_output) # 1 x 100 x 92\n",
        "boxes = model.bbox_predictor(sequence_output).sigmoid() # 1 x 100 x 4\n",
        "probabilities = torch.nn.functional.softmax(logits, -1) # 1 x 100 x 92\n",
        "scores, labels = probabilities[..., :-1].max(-1) # 1 x 100, 1 x 100\n",
        "topK_instance = 10\n",
        "_, indices = scores.topk(topK_instance, sorted=True)"
      ],
      "metadata": {
        "id": "kEL6jgbVTFF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply head to the probes\n",
        "disps = []\n",
        "for i, sequence_output in enumerate(sequence_outputs):\n",
        "\n",
        "    # post-processing, convert outputs (bounding boxes and class logits) to COCO API\n",
        "    logits = model.class_labels_classifier(sequence_output) # 1 x 100 x 92\n",
        "    boxes = model.bbox_predictor(sequence_output).sigmoid() # 1 x 100 x 4\n",
        "    probabilities = torch.nn.functional.softmax(logits, -1) # 1 x 100 x 92\n",
        "    scores, labels = probabilities[..., :-1].max(-1) # 1 x 100, 1 x 100\n",
        "\n",
        "    # let's only keep detections with score > 0.9\n",
        "    selection = torch.zeros(scores.shape,dtype=torch.bool)\n",
        "    selection[:,indices] = True\n",
        "    scores = scores[selection]\n",
        "    labels = labels[selection]\n",
        "    boxes = boxes[selection]\n",
        "    target_sizes = torch.tensor(image.size[::-1])\n",
        "\n",
        "    def convert_boxes(x, width, height):\n",
        "        x_c, y_c, w, h = x.unbind(-1)\n",
        "        b = [(x_c - 0.5 * w)*width, (y_c - 0.5 * h)*height, (x_c + 0.5 * w)*width, (y_c + 0.5 * h)*height]\n",
        "        return torch.stack(b, dim=-1)\n",
        "\n",
        "    # get boxes\n",
        "    boxes = convert_boxes(boxes, target_sizes[1], target_sizes[0])\n",
        "\n",
        "    # visualization\n",
        "    disp = cv2.cvtColor(np.array(image),cv2.COLOR_RGB2BGR)\n",
        "    colors = [ (0,0,255), (0,255,255), (0,255,0), (255,255,0), (255,0,0) ]\n",
        "    for score, label, box, color in zip(scores, labels, boxes, colors):\n",
        "        box = [int(i) for i in box.tolist()]\n",
        "        cv2.rectangle(disp,box[:2],box[2:],color,1)\n",
        "        label_text = model.config.id2label[label.item()]\n",
        "        cv2.putText(disp,label_text,(box[0]+4,box[1]+16),0,0.8,color,1)\n",
        "\n",
        "    disps.append(disp)"
      ],
      "metadata": {
        "id": "C6U4WVKITJ17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the detection progress trough transformations\n",
        "cv2_imshow(cv2.hconcat(disps))"
      ],
      "metadata": {
        "id": "KQvIZPatVBhj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}