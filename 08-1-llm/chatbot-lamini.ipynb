{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch; torch.set_printoptions(precision=3)\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM;\n",
        "import transformers; transformers.logging.set_verbosity_error()\n",
        "import math\n",
        "import numpy as np\n",
        "import warnings; warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "import requests\n",
        "import os\n",
        "import zipfile\n",
        "import io"
      ],
      "metadata": {
        "id": "D5yFz-6HXVQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(repo_id=\"MBZUAI/LaMini-Flan-T5-248M\", local_dir=\"LaMini\")"
      ],
      "metadata": {
        "id": "1HKhAu6sNjWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# details of architecture\n",
        "temperature = 1.0\n",
        "def set_temperature(value):\n",
        "    global temperature\n",
        "    temperature = value\n",
        "\n",
        "def layer_normalization(self, hidden_states):\n",
        "    hidden_states = hidden_states.clone()\n",
        "    variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n",
        "    variance_epsilon = 1e-6\n",
        "    hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n",
        "    normed_hidden_states = self.weight * hidden_states\n",
        "    return normed_hidden_states\n",
        "\n",
        "def attention(self, hidden_states, mask=None, key_value_states=None):\n",
        "    batch_size, seq_length = hidden_states.shape[:2]\n",
        "    key_length = seq_length if key_value_states is None else key_value_states.shape[1]\n",
        "\n",
        "    def shape(states): # projection\n",
        "        return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
        "\n",
        "    def unshape(states): # reshape\n",
        "        return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)\n",
        "\n",
        "    query_states = shape(self.q(hidden_states))\n",
        "    key_states = shape(self.k(hidden_states if key_value_states is None else key_value_states))\n",
        "    value_states = shape(self.v(hidden_states if key_value_states is None else key_value_states))\n",
        "    scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
        "\n",
        "    if not self.has_relative_attention_bias:\n",
        "        position_bias = torch.zeros((1, self.n_heads, seq_length, key_length), device=scores.device, dtype=scores.dtype)\n",
        "    else:\n",
        "        position_bias = self.compute_bias(seq_length, key_length, device=scores.device)\n",
        "\n",
        "    if mask is not None:\n",
        "        position_bias = position_bias + mask\n",
        "\n",
        "    scores += position_bias\n",
        "    attn_weights = torch.nn.functional.softmax(scores.float()/temperature, dim=-1).type_as(scores)\n",
        "    attn_weights = torch.nn.functional.dropout(attn_weights, p=self.dropout, training=False)\n",
        "\n",
        "    attn_output = unshape(torch.matmul(attn_weights, value_states))\n",
        "    attn_output = self.o(attn_output)\n",
        "\n",
        "    return attn_output, position_bias\n",
        "\n",
        "def self_attention(self, hidden_states, mask=None):\n",
        "    return attention(self, hidden_states, mask)\n",
        "\n",
        "def cross_attention(self, hidden_states, mask=None, key_value_states=None):\n",
        "    return attention(self, hidden_states, mask, key_value_states)\n",
        "\n",
        "def GELU(x):\n",
        "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "def FFN(self, hidden_states):\n",
        "    forwarded_states = layer_normalization(self.layer_norm, hidden_states)\n",
        "    hidden_gelu = GELU(self.DenseReluDense.wi_0(forwarded_states))\n",
        "    hidden_linear = self.DenseReluDense.wi_1(forwarded_states)\n",
        "    forwarded_states = hidden_gelu * hidden_linear\n",
        "    forwarded_states = self.DenseReluDense.dropout(forwarded_states)\n",
        "    forwarded_states = self.DenseReluDense.wo(forwarded_states)\n",
        "    hidden_states = hidden_states + self.dropout(forwarded_states)\n",
        "    return hidden_states\n",
        "\n",
        "def T5block(self, hidden_states, mask, encoder_hidden_states=None, encoder_attention_mask=None):\n",
        "    normed_hidden_states = layer_normalization(self.layer[0].layer_norm, hidden_states)\n",
        "    attention_output = self_attention( self.layer[0].SelfAttention,\n",
        "        normed_hidden_states,\n",
        "        mask\n",
        "    )\n",
        "    hidden_states = hidden_states + self.layer[0].dropout(attention_output[0])\n",
        "    attention_outputs = (attention_output[1],)\n",
        "    if self.is_decoder and encoder_hidden_states is not None:\n",
        "        normed_hidden_states = layer_normalization(self.layer[1].layer_norm, hidden_states)\n",
        "        cross_attention_output = cross_attention( self.layer[1].EncDecAttention,\n",
        "            normed_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            encoder_hidden_states\n",
        "        )\n",
        "        hidden_states = hidden_states + self.layer[1].dropout(cross_attention_output[0])\n",
        "        attention_outputs += (cross_attention_output[1],)\n",
        "    hidden_states = FFN(self.layer[-1], hidden_states)\n",
        "    return (hidden_states,) + attention_outputs"
      ],
      "metadata": {
        "id": "WjqPYtp7N7RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chatbot - question and answer\n",
        "checkpoint = \"./LaMini/\"  # LaMini-Flan-T5-248M\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, device='cpu')\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint).to('cpu')\n",
        "set_temperature(1.0)"
      ],
      "metadata": {
        "id": "g3jOsSKMsrvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model"
      ],
      "metadata": {
        "id": "JWPUm_N_s29g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"What is the capital of the USA?\"\n",
        "print(\"Question:\",input_text)"
      ],
      "metadata": {
        "id": "DHCvylYIta72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer - encode\n",
        "tokens = tokenizer.encode(input_text, return_tensors=\"pt\") # shape [1, 9]\n",
        "tokens"
      ],
      "metadata": {
        "id": "3JhuB6fctcku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add start token\n",
        "pad = base_model.config.pad_token_id # 0\n",
        "eos = base_model.config.eos_token_id # 1\n",
        "start = torch.tensor([[pad]])\n",
        "input_tokens = torch.concatenate([start,tokens],dim=1) # shape [1, 10]\n",
        "input_tokens"
      ],
      "metadata": {
        "id": "o_ksU52Wtlb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embed\n",
        "embed = base_model.shared.weight[input_tokens]  # shape [1, 10, 768]\n",
        "embed.shape"
      ],
      "metadata": {
        "id": "BegfDOnEtzj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode\n",
        "def encode(x, mask=None):\n",
        "    for block in base_model.encoder.block:\n",
        "        x, mask = T5block(block, x, mask)\n",
        "    return layer_normalization(base_model.encoder.final_layer_norm, x)\n",
        "\n",
        "hidden = encode(embed) # shapes [1, 10, 768], [1, 12, 10, 10]\n",
        "\n",
        "hidden.shape"
      ],
      "metadata": {
        "id": "kXBUS4Sit0qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate\n",
        "output_tokens = start\n",
        "while True:\n",
        "\n",
        "    # embed\n",
        "    embed = base_model.shared.weight[output_tokens] # shape [1, 1, 768]\n",
        "\n",
        "    # decode\n",
        "    def decode(x, mask, crossx, crossmask):\n",
        "        for block in base_model.decoder.block:\n",
        "            x, mask, crossmask = T5block(block, x, mask, crossx, crossmask)\n",
        "        return layer_normalization(base_model.decoder.final_layer_norm, x)\n",
        "\n",
        "    output = decode(embed, None, hidden, None) # shape [1, N, 768]\n",
        "\n",
        "    # wipe out\n",
        "    logits = torch.matmul(output[0], base_model.lm_head.weight.t())\n",
        "    next_token = torch.argmax(logits[-1,:]) # 0-32127\n",
        "\n",
        "    # add the next token\n",
        "    output_tokens = torch.concatenate([output_tokens,torch.tensor([[next_token]])],dim=1)\n",
        "\n",
        "    print('  ', tokenizer.decode(output_tokens[0], skip_special_tokens=False))\n",
        "\n",
        "    if next_token == eos:\n",
        "        break"
      ],
      "metadata": {
        "id": "pVR3AWJqOO-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "id": "XSwkGFQZzzzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens.shape"
      ],
      "metadata": {
        "id": "dGJ4QGazu7i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer - decode\n",
        "output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "print(\"Answer:\", output_text) # 'Washington, D.C.'"
      ],
      "metadata": {
        "id": "EuNLCimNuT9e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}