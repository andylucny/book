{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cbed182",
      "metadata": {
        "id": "9cbed182"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7af60a5b",
      "metadata": {
        "id": "7af60a5b"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "num_classes = 10\n",
        "learning_rate = 0.001\n",
        "step_size = 16\n",
        "gamma = 0.7\n",
        "weight_decay = 0.00001\n",
        "num_epochs = 70\n",
        "model_name = 'pytorch_cifar10_model.pth'\n",
        "device = 'cuda:0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2ab8ec3",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "b2ab8ec3"
      },
      "outputs": [],
      "source": [
        "# The data, shuffled and split between train and test sets:\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))])\n",
        "train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "728325fa",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "728325fa"
      },
      "outputs": [],
      "source": [
        "# one-hot encoding\n",
        "def to_categorical_all(categories):\n",
        "    num_items = len(categories)\n",
        "    binary_categories = torch.zeros((num_items,num_classes),dtype=torch.float)\n",
        "    for i in range(num_items):\n",
        "        binary_categories[i,categories[i].item()] = 1\n",
        "    return binary_categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5d78d9b",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "b5d78d9b"
      },
      "outputs": [],
      "source": [
        "# network definition\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3,32,(3,3),padding=1), # B x 3 x 32 x 32 -> B x 32 x 32 x 32\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32,32,(3,3)),          # B x 32 x 32 x 32 -> B x 32 x 30 x 30\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                 # B x 32 x 30 x 30 -> B x 32 x 15 x 15\n",
        "            nn.Dropout(0.25),\n",
        "            #----------------\n",
        "            nn.Conv2d(32,64,(3,3),padding=1),# B x 32 x 15 x 15 -> B x 64 x 15 x 15\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64,64,(3,3)),          # B x 64 x 15 x 15 -> B x 64 x 13 x 13\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                 # B x 64 x 13 x 13 -> B x 64 x 6 x 6\n",
        "            nn.Dropout(0.25),\n",
        "            #----------------\n",
        "            nn.Conv2d(64,64,(3,3),padding=1),# B x 64 x 6 x 6 -> B x 64 x 6 x 6\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64,64,(3,3)),          # B x 64 x 6 x 6 -> B x 64 x 4 x 4\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                 # B x 64 x 4 x 4 -> B x 64 x 2 x 2\n",
        "            nn.Dropout(0.25),\n",
        "            #----------------\n",
        "            nn.Flatten()                     # B x 64 x 2 x 2 -> 256\n",
        "        )\n",
        "        self.perceptron = nn.Sequential(\n",
        "            nn.Linear(256,512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512,num_classes),\n",
        "            #nn.Softmax(1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        features = self.encoder(x)\n",
        "        return self.perceptron(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d3a7e65",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "8d3a7e65"
      },
      "outputs": [],
      "source": [
        "model = Classifier().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8c7618d",
      "metadata": {
        "id": "f8c7618d"
      },
      "outputs": [],
      "source": [
        "# Apply Xavier initialization\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "#\n",
        "model.apply(initialize_weights)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss funcion\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "qvK_-N2XnyZR"
      },
      "id": "qvK_-N2XnyZR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a55c6cb7",
      "metadata": {
        "id": "a55c6cb7"
      },
      "outputs": [],
      "source": [
        "# Define optimizer\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7abf9215",
      "metadata": {
        "id": "7abf9215"
      },
      "outputs": [],
      "source": [
        "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c2e17f6",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "7c2e17f6"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "history_loss = []\n",
        "history_acc = []\n",
        "history_test_acc = []\n",
        "epoch = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc22c892",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "fc22c892"
      },
      "outputs": [],
      "source": [
        "def train(num_epochs):\n",
        "    global epoch\n",
        "    for _ in range(num_epochs):\n",
        "        # change model in training mood\n",
        "        model.train()\n",
        "\n",
        "        # to record loss and accuracy\n",
        "        batch_loss = []\n",
        "        total_train = 0\n",
        "        correct_train = 0\n",
        "\n",
        "        for batch, (x_train, y_train) in enumerate(train_loader):\n",
        "\n",
        "            # send data to device\n",
        "            input = x_train.to(device)\n",
        "\n",
        "            # reset parameters gradient to zero\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward pass to the model\n",
        "            output = model(input)\n",
        "\n",
        "            # categorization\n",
        "            expected_output = y_train.to(device)\n",
        "\n",
        "            # cross entropy loss\n",
        "            loss = criterion(output, expected_output)\n",
        "\n",
        "            # find gradients\n",
        "            loss.backward()\n",
        "            # update parameters using gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            # recording loss\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "            # recording accuracy\n",
        "            total_train += output.shape[0]\n",
        "            correct_train += torch.argmax(output,dim=1).to('cpu').eq(y_train).sum().item()\n",
        "\n",
        "        epoch_loss = np.average(batch_loss)\n",
        "        epoch_acc = (100.0 * correct_train) / total_train\n",
        "\n",
        "        history_loss.append(epoch_loss)\n",
        "        history_acc.append(epoch_acc)\n",
        "\n",
        "        total_test = 0\n",
        "        correct_test = 0\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        for batch, (x_test, y_test) in enumerate(test_loader):\n",
        "\n",
        "            # send data to device\n",
        "            input = x_test.to(device)\n",
        "\n",
        "            # forward pass to the model\n",
        "            with torch.no_grad():\n",
        "                output = model(input)\n",
        "\n",
        "            # recording accuracy\n",
        "            total_test += output.shape[0]\n",
        "            correct_test += torch.argmax(output,dim=1).to('cpu').eq(y_test).sum().item()\n",
        "\n",
        "        test_acc = (100.0 * correct_test) / total_test\n",
        "\n",
        "        history_test_acc.append(test_acc)\n",
        "\n",
        "        print(f'Epoch: {epoch} Loss: {epoch_loss:.6f} Accuracy: {epoch_acc:.4f} Test accuracy: {test_acc:.4f} Learning Rate: {scheduler.get_last_lr()[0]:.7f}')\n",
        "        scheduler.step()\n",
        "        epoch += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdf0df2c",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "fdf0df2c"
      },
      "outputs": [],
      "source": [
        "print('\\nTraining')\n",
        "train(num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3602034",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "d3602034"
      },
      "outputs": [],
      "source": [
        "# Save model and weights\n",
        "def save():\n",
        "    torch.save(model.state_dict(), model_name) # weights only\n",
        "    print('Saved trained model at %s ' % model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e438bde",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "7e438bde"
      },
      "outputs": [],
      "source": [
        "save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e16a9625",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "e16a9625"
      },
      "outputs": [],
      "source": [
        "def validate():\n",
        "    model.eval()\n",
        "    total_train = 0\n",
        "    correct_train = 0\n",
        "    for batch, (x_train, y_train) in enumerate(train_loader):\n",
        "        input = x_train.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(input)\n",
        "        total_train += output.shape[0]\n",
        "        correct_train += torch.argmax(output,dim=1).to('cpu').eq(y_train).sum().item()\n",
        "    acc_train = (100.0 * correct_train) / total_train\n",
        "    print(f'training accuracy: {acc_train:.2f}%')\n",
        "    total_test = 0\n",
        "    correct_test = 0\n",
        "    for batch, (x_test, y_test) in enumerate(test_loader):\n",
        "        input = x_test.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(input)\n",
        "        total_test += output.shape[0]\n",
        "        correct_test += torch.argmax(output,dim=1).to('cpu').eq(y_test).sum().item()\n",
        "    acc_test = (100.0 * correct_test) / total_test\n",
        "    print(f'testing accuracy: {acc_test:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b37bd6ba",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "b37bd6ba"
      },
      "outputs": [],
      "source": [
        "validate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28ca7226",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "28ca7226"
      },
      "outputs": [],
      "source": [
        "# test\n",
        "def test():\n",
        "    dataiter = iter(train_loader)\n",
        "    sample_inputs, sample_outputs = next(dataiter)\n",
        "    model.eval() # switch off dropout\n",
        "    with torch.no_grad():\n",
        "        logits = model(sample_inputs.to(device))\n",
        "        categories = torch.argmax(logits,dim=1)\n",
        "        probabilities = F.softmax(logits,dim=1)\n",
        "    print(sample_outputs)\n",
        "    print(categories)\n",
        "    print(sample_outputs[17],'...',[f\"{probability:.3f}\" for probability in probabilities[17]])\n",
        "    print(sample_outputs[17],'...',[f\"{probability:.3f}\" for probability in to_categorical_all(sample_outputs)[17]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5238c0fe",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "5238c0fe"
      },
      "outputs": [],
      "source": [
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a2b66f8",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "5a2b66f8"
      },
      "outputs": [],
      "source": [
        "def history():\n",
        "    # optional only:\n",
        "    import matplotlib.pyplot as plt\n",
        "    # Loss Curves\n",
        "    plt.figure(figsize=[8,6])\n",
        "    plt.plot(history_loss,'r',linewidth=3.0)\n",
        "    plt.legend(['Training loss'],fontsize=18)\n",
        "    plt.xlabel('Epochs ',fontsize=16)\n",
        "    plt.ylabel('Loss',fontsize=16)\n",
        "    plt.title('Loss Curves',fontsize=16)\n",
        "    plt.savefig('fig1.png')\n",
        "    # Accuracy Curves\n",
        "    plt.figure(figsize=[8,6])\n",
        "    plt.plot(history_acc,'r',linewidth=3.0)\n",
        "    plt.plot(history_test_acc,'b',linewidth=3.0)\n",
        "    plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
        "    plt.xlabel('Epochs ',fontsize=16)\n",
        "    plt.ylabel('Accuracy',fontsize=16)\n",
        "    plt.title('Accuracy Curves',fontsize=16)\n",
        "    plt.savefig('fig2.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd5f8741",
      "metadata": {
        "id": "fd5f8741"
      },
      "outputs": [],
      "source": [
        "history()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8675d85",
      "metadata": {
        "id": "b8675d85"
      },
      "outputs": [],
      "source": [
        "# download model\n",
        "from google.colab import files\n",
        "files.download('fig1.png')\n",
        "files.download('fig2.png')\n",
        "files.download('pytorch_cifar10_model.pth')"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}