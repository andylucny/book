{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import io\n",
        "import requests\n",
        "import zipfile\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "D5yFz-6HXVQo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "6ZrOuE-NW_Lh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_zipfile(path,url):\n",
        "    if os.path.exists(path):\n",
        "        return\n",
        "    print(\"downloading\",url)\n",
        "    response = requests.get(url)\n",
        "    if response.ok:\n",
        "        file_like_object = io.BytesIO(response.content)\n",
        "        zipfile_object = zipfile.ZipFile(file_like_object)\n",
        "        zipfile_object.extractall(\".\", pwd=b\"radost\")\n",
        "        print(\"downloaded\")\n",
        "    else:\n",
        "        print(\"download failed\")\n",
        "\n",
        "def download_glee():\n",
        "    download_zipfile('GLEEmodel_swin_complete.pth','http://www.agentspace.org/download/GLEEmodel_swin_complete.zip')\n",
        "\n",
        "download_glee()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh9E9Ujpqvun",
        "outputId": "d0a51c0f-ecb1-421e-d259-204998867bd6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading http://www.agentspace.org/download/GLEEmodel_swin_complete.zip\n",
            "downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('GLEEmodel_swin_complete.pth', weights_only=False).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "Rf2nHm1Zseyt",
        "outputId": "8579d292-d553-42ff-9486-cb6b7036c0ac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL GLEE.glee.models.glee_model.GLEE_Model was not an allowed global by default. Please use `torch.serialization.add_safe_globals([GLEE.glee.models.glee_model.GLEE_Model])` or the `torch.serialization.safe_globals([GLEE.glee.models.glee_model.GLEE_Model])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-194823413.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GLEEmodel_swin_complete.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1527\u001b[0m                         )\n\u001b[1;32m   1528\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1530\u001b[0m                 return _load(\n\u001b[1;32m   1531\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL GLEE.glee.models.glee_model.GLEE_Model was not an allowed global by default. Please use `torch.serialization.add_safe_globals([GLEE.glee.models.glee_model.GLEE_Model])` or the `torch.serialization.safe_globals([GLEE.glee.models.glee_model.GLEE_Model])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O \"twocats.jpg\" \"http://images.cocodataset.org/val2017/000000039769.jpg\""
      ],
      "metadata": {
        "id": "3jJmAXh4oszO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgpath='twocats.jpg'"
      ],
      "metadata": {
        "id": "H9L0G-9hXgzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread(imgpath)\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "vXyx3TxskzxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "Kw2Apxnys3G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(-1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=-1)\n",
        "\n",
        "def LSJ_box_postprocess( out_bbox,  padding_size, crop_size, img_h, img_w): # postprocess box height and width\n",
        "    boxes = box_cxcywh_to_xyxy(out_bbox)\n",
        "    lsj_scale = torch.tensor([padding_size[1], padding_size[0], padding_size[1], padding_size[0]]).to(out_bbox)\n",
        "    crop_scale = torch.tensor([crop_size[1], crop_size[0], crop_size[1], crop_size[0]]).to(out_bbox)\n",
        "    boxes = boxes * lsj_scale\n",
        "    boxes = boxes / crop_scale\n",
        "    boxes = torch.clamp(boxes,0,1)\n",
        "    scale_fct = torch.tensor([img_w, img_h, img_w, img_h])\n",
        "    scale_fct = scale_fct.to(out_bbox)\n",
        "    boxes = boxes * scale_fct\n",
        "    return boxes"
      ],
      "metadata": {
        "id": "mbpfAXN6sPdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_expressions = [ \"the first sleeping cat from the right side\" ]\n",
        "prompt_list = {'grounding':input_expressions}\n",
        "task=\"grounding\""
      ],
      "metadata": {
        "id": "cSvASPYps0eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing\n",
        "copyed_img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "print('input shape',copyed_img.shape)\n",
        "pixel_mean = torch.Tensor( [123.675, 116.28, 103.53]).to(device).view(3, 1, 1)\n",
        "pixel_std = torch.Tensor([58.395, 57.12, 57.375]).to(device).view(3, 1, 1)\n",
        "normalizer = lambda x: (x - pixel_mean) / pixel_std\n",
        "inference_size = 800\n",
        "resizer = torchvision.transforms.Resize(inference_size,antialias=True)\n",
        "size_divisibility = 32\n",
        "ori_image = torch.as_tensor(np.ascontiguousarray( copyed_img.transpose(2, 0, 1)))\n",
        "ori_image = normalizer(ori_image.to(device))[None,]\n",
        "_,_, ori_height, ori_width = ori_image.shape\n",
        "resize_image = resizer(ori_image)\n",
        "image_size = torch.as_tensor((resize_image.shape[-2],resize_image.shape[-1]))\n",
        "re_size = resize_image.shape[-2:]\n",
        "if size_divisibility > 1:\n",
        "    stride = size_divisibility\n",
        "    padding_size = ((image_size + (stride - 1)).div(stride, rounding_mode=\"floor\") * stride).tolist()\n",
        "    infer_image = torch.zeros(1,3,padding_size[0],padding_size[1]).to(resize_image)\n",
        "    infer_image[0,:,:image_size[0],:image_size[1]] = resize_image\n",
        "    infer_image = infer_image.to(device)"
      ],
      "metadata": {
        "id": "dYXEzySkttlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading model into GPU\n",
        "t0 = time.time()\n",
        "dummy = torch.rand(infer_image.shape).to(device)\n",
        "with torch.no_grad():\n",
        "    (outputs,_) = model(dummy, prompt_list, task=task, batch_name_list=[], is_train=False)\n",
        "t1 = time.time()\n",
        "print(f'model loaded in {t1-t0}s')"
      ],
      "metadata": {
        "id": "qNvEPx61twbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_select=['box', 'mask', 'name', 'score', 'expression']"
      ],
      "metadata": {
        "id": "MxdfjE27t5-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run model\n",
        "t0 = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    (outputs,_) = model(infer_image, prompt_list, task=\"grounding\", batch_name_list=[], is_train=False)\n",
        "\n",
        "mask_pred = outputs['pred_masks'][0].to('cpu')\n",
        "mask_cls = outputs['pred_logits'][0].to('cpu')\n",
        "boxes_pred = outputs['pred_boxes'][0].to('cpu')\n",
        "\n",
        "t1 = time.time()\n",
        "print('elapsed',t1-t0,'s')"
      ],
      "metadata": {
        "id": "CzBc2FEAuAKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# alternatively: run model in details\n",
        "t0 = time.time()\n",
        "\n",
        "torch._C._set_grad_enabled(False) # permanent with torch.no_grad()\n",
        "self = model\n",
        "\n",
        "extra =  {}\n",
        "early_semantic = None\n",
        "print('images:',infer_image.shape)\n",
        "print('prompts:',prompt_list)\n",
        "print('task:',task)\n",
        "\n",
        "# encode the textual prompt, text_encoder is CLIP text encoder transformer\n",
        "tokens = self.tokenizer(prompt_list['grounding'], padding='max_length', truncation=True, max_length=self.cfg.MODEL.LANGUAGE_BACKBONE.MAX_QUERY_LEN, return_tensors='pt')\n",
        "tokens = {key: value.to(infer_image.device) for key, value in tokens.items()}\n",
        "x = self.text_encoder(tokens['input_ids'], tokens['attention_mask']) # 1 x 77, 1 x 77 -> 1 x 77 x 512\n",
        "token_x = x['last_hidden_state'] # 1 x 77 x 512\n",
        "token_x = token_x @ self.lang_projection # 1 x 77 x 256\n",
        "extra['grounding_tokens'] = token_x.permute(1,0,2) # 77 x 1 x 256\n",
        "#\n",
        "non_zero_query_mask = tokens['attention_mask']\n",
        "embedded = token_x * non_zero_query_mask.unsqueeze(-1).float() # 1 x 77 x 256, zeros over count of tokens\n",
        "lang_feat_pool = embedded.sum(1) / (non_zero_query_mask.sum(-1).unsqueeze(-1).float()) # 1 x 256, average features\n",
        "#\n",
        "extra['grounding_nonzero_mask'] = ~non_zero_query_mask.bool()  # 1 x 77 True for zeros\n",
        "extra['grounding_class'] = lang_feat_pool.squeeze(1) # 1 x 256\n",
        "early_semantic = {\"hidden\":token_x.float(),\"masks\":tokens['attention_mask']>0} # 1 x 77 x 256, 1 x\n",
        "\n",
        "# encode the image, backbone is the swin transformer\n",
        "features = self.backbone(infer_image) # 1 x 3 x 800 x 960 -> res2: 1 x 192 x 200 x 240, res3: 1 x 384 x 100 x 120, res4: 1 x 768 x 50 x 60, res5: 1 x 1536 x 25 x 30\n",
        "\n",
        "# decode mask and box features, pixel_decoder is Mask-DINO encoder\n",
        "mask_features, _, multi_scale_features, _ = self.pixel_decoder.forward_features(features, masks=None, early_fusion = early_semantic)\n",
        "# -> 1 x 256 x 200 x 240, _, [ 1 x 256 x 100 x 120, 1 x 256 x 50 x 60, 1 x 256 x 25 x 30, 1 x 256 x 13 x 15 ], _\n",
        "\n",
        "# predict masks, classes and boxes, predictor is Mask-DINO decoder\n",
        "outputs, _ = self.predictor(multi_scale_features, mask_features, extra=extra, task=task, masks=None, targets=None)\n",
        "# -> 'pred_logits'     : 1 x 300 x 1\n",
        "#    'pred_scores'     : 1 x 300 x 1\n",
        "#    'pred_masks'      : 1 x 300 x 200 x 240\n",
        "#    'pred_boxes'      : 1 x 300 x 4\n",
        "#    'pred_track_embed': 1 x 300 x 256\n",
        "\n",
        "mask_pred = outputs['pred_masks'][0].to('cpu')\n",
        "mask_cls = outputs['pred_logits'][0].to('cpu')\n",
        "boxes_pred = outputs['pred_boxes'][0].to('cpu')\n",
        "\n",
        "t1 = time.time()\n",
        "print('elapsed',t1-t0,'s')"
      ],
      "metadata": {
        "id": "ru1aoQTXxDsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# postprocessing\n",
        "scores = mask_cls.sigmoid().max(-1)[0]\n",
        "topK_instance = 1\n",
        "scores_per_image, topk_indices = scores.topk(topK_instance, sorted=True)\n",
        "\n",
        "pred_class = mask_cls[topk_indices].max(-1)[1].tolist()\n",
        "pred_boxes = boxes_pred[topk_indices]\n",
        "\n",
        "boxes = LSJ_box_postprocess(pred_boxes,padding_size,re_size, ori_height,ori_width)\n",
        "mask_pred = mask_pred[topk_indices]\n",
        "pred_masks = F.interpolate( mask_pred[None,], size=(padding_size[0], padding_size[1]), mode=\"bilinear\", align_corners=False  )\n",
        "pred_masks = pred_masks[:,:,:re_size[0],:re_size[1]]\n",
        "pred_masks = F.interpolate( pred_masks, size=(ori_height,ori_width), mode=\"bilinear\", align_corners=False  )\n",
        "pred_masks = (pred_masks>0).detach().cpu().numpy()[0]"
      ],
      "metadata": {
        "id": "Y-mDYwqTuP8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualization\n",
        "COLORS = [\n",
        "    [0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "    [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933],\n",
        "    [0.494, 0.000, 0.556], [0.494, 0.000, 0.000], [0.000, 0.745, 0.000],\n",
        "    [0.700, 0.300, 0.600], [0.000, 0.447, 0.741], [0.850, 0.325, 0.098]\n",
        "]\n",
        "\n",
        "zero_mask = np.zeros_like(copyed_img)\n",
        "for nn, mask in enumerate(pred_masks):\n",
        "    mask = mask.reshape(mask.shape[0], mask.shape[1], 1)\n",
        "    lar = np.concatenate((mask*COLORS[nn%12][2], mask*COLORS[nn%12][1], mask*COLORS[nn%12][0]), axis = 2)\n",
        "    zero_mask = zero_mask+ lar\n",
        "\n",
        "lar_valid = zero_mask>0\n",
        "masked_image = lar_valid*copyed_img\n",
        "mask_image_mix_ration = 0.65\n",
        "img_n = masked_image*mask_image_mix_ration + np.clip(zero_mask,0,1)*255*(1-mask_image_mix_ration)\n",
        "max_p = img_n.max()\n",
        "img_n = 255*img_n/max_p\n",
        "ret = (~lar_valid*copyed_img)*mask_image_mix_ration + img_n\n",
        "ret = ret.astype('uint8')\n",
        "retimg = cv2.cvtColor(ret,cv2.COLOR_RGB2BGR)"
      ],
      "metadata": {
        "id": "qtJfzmURsG03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(retimg)"
      ],
      "metadata": {
        "id": "S-MknonzuhJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = pred_masks[0]\n",
        "mask = mask.reshape(mask.shape[0], mask.shape[1], 1)\n",
        "mask = mask.astype(np.uint8)\n",
        "mask = np.squeeze(mask)\n",
        "mask *= 255"
      ],
      "metadata": {
        "id": "sABOGOJuua4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(mask)"
      ],
      "metadata": {
        "id": "rSY4vb4qupvT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}