{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "D5yFz-6HXVQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "6ZrOuE-NW_Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"ambityga/imagenet100\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "HGBatKFKutJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l $path/val.X/n01773549"
      ],
      "metadata": {
        "id": "PQhrNEdVu4DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imagepath = path + '/val.X/' + 'n01773549' +'/' + 'ILSVRC2012_val_00008316.JPEG'"
      ],
      "metadata": {
        "id": "l0zeZwsYvNgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "image = cv2.imread(imagepath)\n",
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(image)"
      ],
      "metadata": {
        "id": "vXyx3TxskzxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from types import SimpleNamespace\n",
        "config = SimpleNamespace()\n",
        "config.classifier = \"token\"\n",
        "config.hidden_size = 768\n",
        "config.patches = {\n",
        "    \"grid\": (32, 32),\n",
        "}\n",
        "config.transformer = {\n",
        "    \"num_heads\": 12,\n",
        "    \"num_layers\": 12,\n",
        "    \"attention_dropout_rate\": 0.0,\n",
        "    \"dropout_rate\": 0.1,\n",
        "    \"mlp_dim\": 3072,\n",
        "}"
      ],
      "metadata": {
        "id": "ft3tvmu_NpfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def np2th(weights, conv=False): # convert HWIO to OIHW\n",
        "    \"\"\"Possibly .\"\"\"\n",
        "    if conv:\n",
        "        weights = weights.transpose([3, 2, 0, 1])\n",
        "    return torch.from_numpy(weights)"
      ],
      "metadata": {
        "id": "K3J8_NlrMerK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}"
      ],
      "metadata": {
        "id": "ZQeKaTCvMsDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "from torch.nn import Linear, Dropout, LayerNorm, Softmax\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Attention, self).__init__()\n",
        "        self.vis = vis\n",
        "        self.num_attention_heads = config.transformer[\"num_heads\"]\n",
        "        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.out = Linear(config.hidden_size, config.hidden_size)\n",
        "        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
        "        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
        "\n",
        "        self.softmax = Softmax(dim=-1)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_probs = self.softmax(attention_scores)\n",
        "        weights = attention_probs if self.vis else None\n",
        "        attention_probs = self.attn_dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        attention_output = self.out(context_layer)\n",
        "        attention_output = self.proj_dropout(attention_output)\n",
        "        return attention_output, weights"
      ],
      "metadata": {
        "id": "4l39zrJBO7Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mlp(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Mlp, self).__init__()\n",
        "        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n",
        "        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n",
        "        self.act_fn = ACT2FN[\"gelu\"]\n",
        "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
        "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "IzFNJMwTO82k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.utils import _pair\n",
        "from torch.nn import Conv2d\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, config, img_size, in_channels=3):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.hybrid = None\n",
        "        img_size = _pair(img_size)\n",
        "\n",
        "        if config.patches.get(\"grid\") is not None:\n",
        "            grid_size = config.patches[\"grid\"]\n",
        "            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n",
        "            n_patches = (img_size[0] // 16) * (img_size[1] // 16)\n",
        "            self.hybrid = True\n",
        "        else:\n",
        "            patch_size = _pair(config.patches[\"size\"])\n",
        "            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
        "            self.hybrid = False\n",
        "\n",
        "        self.patch_embeddings = Conv2d(in_channels=in_channels, out_channels=config.hidden_size, kernel_size=patch_size, stride=patch_size)\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches+1, config.hidden_size))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
        "\n",
        "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "\n",
        "        if self.hybrid:\n",
        "            x = self.hybrid_model(x)\n",
        "        x = self.patch_embeddings(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(-1, -2)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        embeddings = x + self.position_embeddings\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "TUqfOCk2PEUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\n",
        "ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n",
        "ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n",
        "ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n",
        "FC_0 = \"MlpBlock_3/Dense_0\"\n",
        "FC_1 = \"MlpBlock_3/Dense_1\"\n",
        "ATTENTION_NORM = \"LayerNorm_0\"\n",
        "MLP_NORM = \"LayerNorm_2\""
      ],
      "metadata": {
        "id": "q7Kr3NItMmsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pjoin(a,b,c):\n",
        "    return a + \"/\" + b + \"/\" + c\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Block, self).__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.ffn = Mlp(config)\n",
        "        self.attn = Attention(config, vis)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        x = self.attention_norm(x)\n",
        "        x, weights = self.attn(x)\n",
        "        x = x + h\n",
        "\n",
        "        h = x\n",
        "        x = self.ffn_norm(x)\n",
        "        x = self.ffn(x)\n",
        "        x = x + h\n",
        "        return x, weights\n",
        "\n",
        "    def load_from(self, weights, n_block):\n",
        "        ROOT = f\"Transformer/encoderblock_{n_block}\"\n",
        "        with torch.no_grad():\n",
        "            query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "\n",
        "            query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n",
        "            key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n",
        "            value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n",
        "            out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n",
        "\n",
        "            self.attn.query.weight.copy_(query_weight)\n",
        "            self.attn.key.weight.copy_(key_weight)\n",
        "            self.attn.value.weight.copy_(value_weight)\n",
        "            self.attn.out.weight.copy_(out_weight)\n",
        "            self.attn.query.bias.copy_(query_bias)\n",
        "            self.attn.key.bias.copy_(key_bias)\n",
        "            self.attn.value.bias.copy_(value_bias)\n",
        "            self.attn.out.bias.copy_(out_bias)\n",
        "\n",
        "            mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n",
        "            mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n",
        "            mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n",
        "            mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n",
        "\n",
        "            self.ffn.fc1.weight.copy_(mlp_weight_0)\n",
        "            self.ffn.fc2.weight.copy_(mlp_weight_1)\n",
        "            self.ffn.fc1.bias.copy_(mlp_bias_0)\n",
        "            self.ffn.fc2.bias.copy_(mlp_bias_1)\n",
        "\n",
        "            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n",
        "            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n",
        "            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n",
        "            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))"
      ],
      "metadata": {
        "id": "7P-ZJ77FPYa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vis = vis\n",
        "        self.layer = nn.ModuleList()\n",
        "        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        for _ in range(config.transformer[\"num_layers\"]):\n",
        "            layer = Block(config, vis)\n",
        "            self.layer.append(copy.deepcopy(layer))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        attn_weights = []\n",
        "        for layer_block in self.layer:\n",
        "            hidden_states, weights = layer_block(hidden_states)\n",
        "            if self.vis:\n",
        "                attn_weights.append(weights)\n",
        "        encoded = self.encoder_norm(hidden_states)\n",
        "        return encoded, attn_weights"
      ],
      "metadata": {
        "id": "quAN1xasPfbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config, img_size, vis):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embeddings = Embeddings(config, img_size=img_size)\n",
        "        self.encoder = Encoder(config, vis)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embedding_output = self.embeddings(input_ids)\n",
        "        encoded, attn_weights = self.encoder(embedding_output)\n",
        "        return encoded, attn_weights"
      ],
      "metadata": {
        "id": "EmhxuTV0Pi1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "from scipy import ndimage\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, config, img_size=224, num_classes=21843, zero_head=False, vis=False):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.zero_head = zero_head\n",
        "        self.classifier = config.classifier\n",
        "\n",
        "        self.transformer = Transformer(config, img_size, vis)\n",
        "        self.head = Linear(config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        x, attn_weights = self.transformer(x)\n",
        "        logits = self.head(x[:, 0])\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n",
        "            return loss\n",
        "        else:\n",
        "            return logits, attn_weights\n",
        "\n",
        "    def load_from(self, weights):\n",
        "        with torch.no_grad():\n",
        "            if self.zero_head:\n",
        "                nn.init.zeros_(self.head.weight)\n",
        "                nn.init.zeros_(self.head.bias)\n",
        "            else:\n",
        "                self.head.weight.copy_(np2th(weights[\"head/kernel\"]).t())\n",
        "                self.head.bias.copy_(np2th(weights[\"head/bias\"]).t())\n",
        "\n",
        "            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n",
        "            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n",
        "            self.transformer.embeddings.cls_token.copy_(np2th(weights[\"cls\"]))\n",
        "            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
        "            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
        "\n",
        "            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
        "            posemb_new = self.transformer.embeddings.position_embeddings\n",
        "            if posemb.size() == posemb_new.size():\n",
        "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
        "            else:\n",
        "                print(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n",
        "                ntok_new = posemb_new.size(1)\n",
        "\n",
        "                if self.classifier == \"token\":\n",
        "                    posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
        "                    ntok_new -= 1\n",
        "                else:\n",
        "                    posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
        "\n",
        "                gs_old = int(np.sqrt(len(posemb_grid)))\n",
        "                gs_new = int(np.sqrt(ntok_new))\n",
        "                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n",
        "                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
        "\n",
        "                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
        "                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)\n",
        "                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n",
        "                posemb = np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
        "                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n",
        "\n",
        "            for bname, block in self.transformer.encoder.named_children():\n",
        "                for uname, unit in block.named_children():\n",
        "                    unit.load_from(weights, n_block=uname)\n",
        "\n",
        "            if self.transformer.embeddings.hybrid:\n",
        "                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(weights[\"conv_root/kernel\"], conv=True))\n",
        "                gn_weight = np2th(weights[\"gn_root/scale\"]).view(-1)\n",
        "                gn_bias = np2th(weights[\"gn_root/bias\"]).view(-1)\n",
        "                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n",
        "                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n",
        "\n",
        "                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n",
        "                    for uname, unit in block.named_children():\n",
        "                        unit.load_from(weights, n_block=bname, n_unit=uname)"
      ],
      "metadata": {
        "id": "XzZR7NQcLhBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mimic module hierarchy from the pretrained model\n",
        "import sys\n",
        "import types\n",
        "\n",
        "# Create fake package \"models\"\n",
        "models = types.ModuleType(\"models\")\n",
        "sys.modules[\"models\"] = models\n",
        "\n",
        "# Create fake submodule \"models.modeling\"\n",
        "modeling = types.ModuleType(\"models.modeling\")\n",
        "sys.modules[\"models.modeling\"] = modeling\n",
        "\n",
        "# Expose your classes inside the fake module\n",
        "modeling.VisionTransformer = VisionTransformer\n",
        "modeling.Transformer = Transformer\n",
        "modeling.Encoder = Encoder\n",
        "modeling.Block = Block\n",
        "modeling.Embeddings = Embeddings\n",
        "modeling.Attention = Attention\n",
        "modeling.Mlp = Mlp"
      ],
      "metadata": {
        "id": "A5fo3C7cLCh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the pretrained model\n",
        "!wget http://www.agentspace.org/download/ViT-B_32.pth"
      ],
      "metadata": {
        "id": "dz81ZU7CXI0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model\n",
        "model = torch.load('ViT-B_32.pth', weights_only=False)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "3dfrD8aAl5II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load image\n",
        "from PIL import Image\n",
        "im = Image.open(imagepath)"
      ],
      "metadata": {
        "id": "j-tSe4XxWRxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "blob = transform(im).unsqueeze(0).to(device)\n",
        "print('image', blob.shape)"
      ],
      "metadata": {
        "id": "xNOEDwApWaKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the transformer model\n",
        "def embed(self, x):\n",
        "    B = x.shape[0]\n",
        "    cls_tokens = self.cls_token.expand(B, -1, -1) # nn.Parameters  1 x 1 x 768\n",
        "\n",
        "    if self.hybrid:\n",
        "        x = self.hybrid_model(x)\n",
        "\n",
        "    x = self.patch_embeddings(x) # Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32)) -> 1 x 768 x 7 x 7\n",
        "    x = x.flatten(2) # 1 x 768 x 49\n",
        "    x = x.transpose(-1, -2) # 1 x 49 x 768\n",
        "    x = torch.cat((cls_tokens, x), dim=1) # 1 x 50 x 768\n",
        "\n",
        "    embeddings = x + self.position_embeddings #  1 x 50 x 768\n",
        "    embeddings = self.dropout(embeddings)\n",
        "    return embeddings\n",
        "\n",
        "hidden_states = embed(model.transformer.embeddings, blob) # 1 x 50 x 768\n",
        "\n",
        "def encode(self, hidden_states):\n",
        "    attn_maps = []\n",
        "    for layer_block in self.layer:\n",
        "        hidden_states, coefs = layer_block(hidden_states)\n",
        "        if self.vis:\n",
        "            attn_maps.append(coefs)\n",
        "\n",
        "    encoded = self.encoder_norm(hidden_states)\n",
        "    return encoded, attn_maps\n",
        "\n",
        "hidden_states, att_maps = encode(model.transformer.encoder, hidden_states) # 1 x 50 x 768, 12 x 50 x 50\n",
        "\n",
        "def lmhead(self, hidden_states):\n",
        "    logits = self.head(hidden_states[:, 0]) # Linear(in_features=768, out_features=1000, bias=True) 1 x 1 x 768 -> 1 x 1000\n",
        "    return logits\n",
        "\n",
        "logits = lmhead(model, hidden_states)\n",
        "\n",
        "print('logits',logits.shape)\n",
        "print('att maps',[att_map.shape for att_map in att_maps])"
      ],
      "metadata": {
        "id": "D4LxH_B4WrFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download ImageNet Labels\n",
        "!wget http://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt"
      ],
      "metadata": {
        "id": "_7UmyaFbXObe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imagenet_labels = dict(enumerate(open('ilsvrc2012_wordnet_lemmas.txt')))\n",
        "print(imagenet_labels)"
      ],
      "metadata": {
        "id": "DBU-I9s-Xky7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Present probabilities of categories\n",
        "probs = torch.nn.Softmax(dim=-1)(logits)\n",
        "top5 = torch.argsort(probs, dim=-1, descending=True)\n",
        "\n",
        "print(\"Prediction:\")\n",
        "for idx in top5[0, :5]:\n",
        "    print(f'{probs[0, idx.item()]:.5f} : {imagenet_labels[idx.item()]}', end='')"
      ],
      "metadata": {
        "id": "HNiUhdYkWqGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_mask(img, mask):\n",
        "    H, W = img.shape[:2]\n",
        "    mask_resized = cv2.resize(mask, (W, H), interpolation=cv2.INTER_LINEAR)\n",
        "    if img.ndim == 3:\n",
        "        mask_resized = np.repeat(mask_resized[:, :, None], 3, axis=2)\n",
        "    result = (img.astype(np.float32) * mask_resized).clip(0, 255).astype(np.uint8)\n",
        "    return result"
      ],
      "metadata": {
        "id": "j4os_uk5zPWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "base = cv2.cvtColor(np.array(im), cv2.COLOR_RGB2GRAY)\n",
        "att_mats = torch.cat(att_maps) # (12, 12, 50, 50)\n",
        "num_layers, num_heads, N, _ = att_mats.shape\n",
        "grid_size = int(math.sqrt(N-1))\n",
        "plt.figure(figsize=(2 * num_heads, 2 * num_layers))\n",
        "for t in range(num_layers):\n",
        "    for i in range(num_heads):\n",
        "        head = att_mats[t, i]      # shape (50, 50)\n",
        "        mask = head[0, 1:].reshape(grid_size, grid_size)\n",
        "        mask = mask.detach().cpu().numpy()\n",
        "        disp = draw_mask(base, mask)\n",
        "        plt.subplot(num_layers, num_heads, t * num_heads + i + 1)\n",
        "        plt.imshow(disp, cmap='gray')\n",
        "        plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nwRiG6bW1VfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# joint_attention:\n",
        "# ===============\n",
        "print(att_mats.shape) # (num_layers, num_heads, N, N)"
      ],
      "metadata": {
        "id": "ZPSmnh8JLslF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Average attention across heads: (L, N, N)\n",
        "att = att_mats.mean(dim=1)\n",
        "print(att.shape)"
      ],
      "metadata": {
        "id": "jPH6r5ZxSBff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize joined heads\n",
        "plt.figure(figsize=(2, 2 * num_layers))\n",
        "for t in range(num_layers):\n",
        "    head = att[t]\n",
        "    mask = head[0, 1:].reshape(grid_size, grid_size)\n",
        "    mask = mask.detach().cpu().numpy()\n",
        "    disp = draw_mask(base, mask)\n",
        "    plt.subplot(num_layers, 1, t+1)\n",
        "    plt.imshow(disp, cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EfFApJGfRB9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Add residual connection to each layer\n",
        "# (I + A) normalized by rows\n",
        "eye = torch.eye(N).expand(num_layers, N, N).to(att.device)\n",
        "att = att + eye\n",
        "# row-normalize\n",
        "att = att / att.sum(dim=-1, keepdim=True)\n",
        "print(att.shape)"
      ],
      "metadata": {
        "id": "BhSNwXKcSN7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize adjusted heads\n",
        "plt.figure(figsize=(2, 2 * num_layers))\n",
        "for t in range(num_layers):\n",
        "    head = att[t]\n",
        "    mask = head[0, 1:].reshape(grid_size, grid_size)\n",
        "    mask = mask.detach().cpu().numpy()\n",
        "    disp = draw_mask(base, mask)\n",
        "    plt.subplot(num_layers, 1, t+1)\n",
        "    plt.imshow(disp, cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zukqc_5ZS7qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3) Joint attention: multiply attention matrices from first to last layer\n",
        "joint_att = att[0]\n",
        "for l in range(1, num_layers):\n",
        "    joint_att = att[l] @ joint_att  # matrix multiplication\n",
        "\n",
        "print(joint_att.shape)"
      ],
      "metadata": {
        "id": "lZAqwVHPKAQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize joined heads\n",
        "plt.figure(figsize=(2, 2))\n",
        "mask = joint_att[0, 1:].reshape(grid_size, grid_size)\n",
        "mask = mask.detach().cpu().numpy()\n",
        "disp = draw_mask(base, mask)\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.imshow(disp, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kHxmUbcMSi7s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}