{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "D5yFz-6HXVQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "6ZrOuE-NW_Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download some images\n",
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"adityajn105/flickr8k\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "5dOFO01wkgnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ln -sf $path/Images ./images"
      ],
      "metadata": {
        "id": "QwT81Anh-fhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l ./images/"
      ],
      "metadata": {
        "id": "CQ0pfM5EkotO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imagepath = './images/978580450_e862715aba.jpg'"
      ],
      "metadata": {
        "id": "-34V77Czlv_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "image = cv2.imread(imagepath)\n",
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(image)"
      ],
      "metadata": {
        "id": "vXyx3TxskzxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://www.agentspace.org/download/ViT-B_32.pth"
      ],
      "metadata": {
        "id": "dz81ZU7CXI0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model\n",
        "import torch\n",
        "model = torch.load('ViT-B_32.pth', weights_only=False)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "3dfrD8aAl5II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Test Image\n",
        "im = Image.open(\"img.jpg\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "blob = transform(im).unsqueeze(0).to(device)\n",
        "print('image', blob.shape)\n",
        "\n",
        "# Call the transformer model\n",
        "\n",
        "def embed(self, x):\n",
        "    B = x.shape[0]\n",
        "    cls_tokens = self.cls_token.expand(B, -1, -1) # nn.Parameters  1 x 1 x 768\n",
        "\n",
        "    if self.hybrid:\n",
        "        x = self.hybrid_model(x)\n",
        "\n",
        "    x = self.patch_embeddings(x) # Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32)) -> 1 x 768 x 7 x 7\n",
        "    x = x.flatten(2) # 1 x 768 x 49\n",
        "    x = x.transpose(-1, -2) # 1 x 49 x 768\n",
        "    x = torch.cat((cls_tokens, x), dim=1) # 1 x 50 x 768\n",
        "\n",
        "    embeddings = x + self.position_embeddings #  1 x 50 x 768\n",
        "    embeddings = self.dropout(embeddings)\n",
        "    return embeddings\n",
        "\n",
        "hidden_states = embed(model.transformer.embeddings, blob) # 1 x 50 x 768\n",
        "\n",
        "def encode(self, hidden_states):\n",
        "    attn_maps = []\n",
        "    for layer_block in self.layer:\n",
        "        hidden_states, coefs = layer_block(hidden_states)\n",
        "        if self.vis:\n",
        "            attn_maps.append(coefs)\n",
        "\n",
        "    encoded = self.encoder_norm(hidden_states)\n",
        "    return encoded, attn_maps\n",
        "\n",
        "hidden_states, att_maps = encode(model.transformer.encoder, hidden_states) # 1 x 50 x 768, 12 x 50 x 50\n",
        "\n",
        "def lmhead(self, hidden_states):\n",
        "    logits = self.head(hidden_states[:, 0]) # Linear(in_features=768, out_features=1000, bias=True) 1 x 1 x 768 -> 1 x 1000\n",
        "    return logits\n",
        "\n",
        "logits = lmhead(model, hidden_states)\n",
        "\n",
        "print('logits',logits.shape)\n",
        "print('att maps',[att_map.shape for att_map in att_maps])\n",
        "\n",
        "# Present probabilities of categories\n",
        "probs = torch.nn.Softmax(dim=-1)(logits)\n",
        "top5 = torch.argsort(probs, dim=-1, descending=True)\n",
        "imagenet_labels = dict(enumerate(open('checkpoint/ilsvrc2012_wordnet_lemmas.txt')))\n",
        "print(\"Prediction:\")\n",
        "for idx in top5[0, :5]:\n",
        "    print(f'{probs[0, idx.item()]:.5f} : {imagenet_labels[idx.item()]}', end='')\n",
        "\n",
        "# Present attention maps.\n",
        "att_mat = torch.stack(att_maps).squeeze(1)\n",
        "att_mat = torch.mean(att_mat, dim=1) # average through heads\n",
        "att_mat = att_mat / att_mat.sum(dim=-1).unsqueeze(-1) # normalize\n",
        "base = cv2.cvtColor(np.array(im),cv2.COLOR_RGB2GRAY)\n",
        "grid_size = int(np.sqrt(att_mat.size(-1)))\n",
        "for i, v in enumerate(att_mat):\n",
        "    mask = v[0, 1:].reshape(grid_size,grid_size).detach().cpu().numpy()\n",
        "    mask = cv2.resize(mask / mask.max(), (base.shape[1],base.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "    mask = (mask*255).astype(np.uint8)\n",
        "    red = np.copy(mask)\n",
        "    red[red < 127] = 0\n",
        "    green = 255 - mask\n",
        "    green[green < 127] = 0\n",
        "    disp = cv2.merge([base,base|green,base|red])\n",
        "    cv2.imwrite(f'outputs/att{i}.png',disp)\n",
        ""
      ],
      "metadata": {
        "id": "D4LxH_B4WrFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fbuLkhlbWqtD"
      }
    }
  ]
}