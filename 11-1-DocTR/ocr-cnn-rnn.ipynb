{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlAfsYNGkmcD"
      },
      "outputs": [],
      "source": [
        "!pip install python-doctr -q\n",
        "!pip install mplcursors -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download an image with text\n",
        "!wget https://github.com/opencv/opencv/blob/master/samples/data/imageTextN.png?raw=true -O sample.jpg\n",
        "#!wget https://www.robots.ox.ac.uk/~vgg/software/textspot/text.png -O sample2.jpg"
      ],
      "metadata": {
        "id": "sWAj8BnNlI-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import doctr\n",
        "from doctr.io import DocumentFile\n",
        "from doctr.models import ocr_predictor\n",
        "from doctr.utils.visualization import visualize_page\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "-CWVRyP2k6U2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(cv2.imread(\"sample.jpg\"))"
      ],
      "metadata": {
        "id": "ltCVU24SnQwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = DocumentFile.from_images(\"sample.jpg\")"
      ],
      "metadata": {
        "id": "df1qwlPbk8At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ocr_predictor(\n",
        "    det_arch=\"db_resnet50\",\n",
        "    reco_arch=\"crnn_vgg16_bn\",\n",
        "    pretrained=True\n",
        ")"
      ],
      "metadata": {
        "id": "awa6dCSolGue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "u7zZWl71u9LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model(doc)"
      ],
      "metadata": {
        "id": "_T0JGPWxnezu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.pages[0].render())"
      ],
      "metadata": {
        "id": "TGhgg4j5oc8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.pages[0].export()"
      ],
      "metadata": {
        "id": "31RwwVY6tyoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize page\n",
        "visualize_page(result.pages[0].export(), doc[0])"
      ],
      "metadata": {
        "id": "jh90ekLVs3I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in details"
      ],
      "metadata": {
        "id": "9wTCiO9ewEqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "U2KvxvgVyG1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "id": "gCpSmWO1wyMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detector = model.det_predictor.model.to(device)"
      ],
      "metadata": {
        "id": "33R2bA6DwHHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread(\"sample.jpg\")"
      ],
      "metadata": {
        "id": "IHVCNFIXxGWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# detect preprocessing\n",
        "def functional_resize(img, size=(1024,1024), preserve_aspect_ratio=True, symmetric_pad=True):\n",
        "    target_ratio = size[0] / size[1]\n",
        "    actual_ratio = img.shape[-2] / img.shape[-1]\n",
        "\n",
        "    if not preserve_aspect_ratio or (target_ratio == actual_ratio):\n",
        "        img = torchvision.transforms.functional.resize(img, size, interpolation=torchvision.transforms.InterpolationMode.BILINEAR, antialias=True)\n",
        "        _pad = (0, 0, 0, 0)\n",
        "    else:\n",
        "        # Resize\n",
        "        if actual_ratio > target_ratio:\n",
        "            tmp_size = (size[0], max(int(size[0] / actual_ratio), 1))\n",
        "        else:\n",
        "            tmp_size = (max(int(size[1] * actual_ratio), 1), size[1])\n",
        "\n",
        "        # Scale image\n",
        "        img = torchvision.transforms.functional.resize(img, tmp_size, interpolation=torchvision.transforms.InterpolationMode.BILINEAR, antialias=True)\n",
        "        raw_shape = img.shape[-2:]\n",
        "\n",
        "        # Pad (inverted in pytorch)\n",
        "        _pad = (0, size[1] - img.shape[-1], 0, size[0] - img.shape[-2])\n",
        "        if symmetric_pad:\n",
        "            half_pad = (math.ceil(_pad[1] / 2), math.ceil(_pad[3] / 2))\n",
        "            _pad = (half_pad[0], _pad[1] - half_pad[0], half_pad[1], _pad[3] - half_pad[1])\n",
        "        # Pad image\n",
        "        img = torch.nn.functional.pad(img, _pad)\n",
        "\n",
        "    return img, _pad\n",
        "\n",
        "def functional_normalize(input_tensor, mean,std):\n",
        "    mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "    std = torch.tensor(std).view(-1, 1, 1)\n",
        "    return (input_tensor - mean) / std\n",
        "\n",
        "def detector_preprocess(page, size=(1024,1024)):\n",
        "    x = torch.from_numpy(page).permute(2, 0, 1)\n",
        "    x, padding = functional_resize(x,(1024,1024))\n",
        "    x = x.to(dtype=torch.float32).div(255).clip(0, 1)\n",
        "    x = functional_normalize(x,mean=(0.798, 0.785, 0.772),std=(0.264, 0.2749, 0.287))\n",
        "    x = x.unsqueeze(0)\n",
        "    return x, padding\n",
        "\n",
        "blob, padding = detector_preprocess(image)\n",
        "\n",
        "print(image.shape, blob.shape, padding)"
      ],
      "metadata": {
        "id": "Go84zPZzwKSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# detect (classify each pixel for being a piece of text, apply threshold and geometry)\n",
        "inp = blob.to(device)\n",
        "feats = detector.feat_extractor(inp)\n",
        "feats = [feats[str(idx)] for idx in range(len(feats))]\n",
        "feat_concat = detector.fpn(feats)\n",
        "logits = detector.prob_head(feat_concat)\n",
        "prob_map = torch.sigmoid(logits)"
      ],
      "metadata": {
        "id": "XIU5p-mvwR2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# detector backbone\n",
        "detector.feat_extractor"
      ],
      "metadata": {
        "id": "5NeC7jhSIaJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[feat.shape for feat in feats]"
      ],
      "metadata": {
        "id": "1aGp9fVmG1Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# detector Feature Pyramid Network (FPN)\n",
        "detector.fpn"
      ],
      "metadata": {
        "id": "wem2syfuJCx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_concat.shape"
      ],
      "metadata": {
        "id": "FOqzd5xJJU3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detector.prob_head"
      ],
      "metadata": {
        "id": "puIkZl7xJLNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape, logits.min().item(), logits.max().item()"
      ],
      "metadata": {
        "id": "vnWlTAWFda2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_map.shape, prob_map.min().item(), prob_map.max().item()"
      ],
      "metadata": {
        "id": "Z9qyf8L1Hjqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow((prob_map[0].squeeze(0).detach().cpu().numpy()*255).astype(np.uint8))"
      ],
      "metadata": {
        "id": "VLOWn5xbqW7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# postprocess\n",
        "def polygon_to_box(points, unclip_ratio = 1.44):\n",
        "\n",
        "    # Ensure the polygon is in float for accuracy\n",
        "    points = points.astype(np.float32)\n",
        "\n",
        "    area = cv2.contourArea(points)\n",
        "    length = cv2.arcLength(points, closed=True)\n",
        "\n",
        "    if length == 0:\n",
        "        return None\n",
        "\n",
        "    distance = area * unclip_ratio / length  # similar to original\n",
        "\n",
        "    # Compute normal directions for each edge\n",
        "    expanded = []\n",
        "    num_points = len(points)\n",
        "\n",
        "    for i in range(num_points):\n",
        "        p1 = points[i]\n",
        "        p2 = points[(i + 1) % num_points]\n",
        "        edge = p2 - p1\n",
        "        edge_length = np.linalg.norm(edge)\n",
        "\n",
        "        if edge_length == 0:\n",
        "            continue\n",
        "\n",
        "        # Normal vector to the edge (clockwise)\n",
        "        normal = np.array([-edge[1], edge[0]]) / edge_length\n",
        "        expanded.append(normal)\n",
        "\n",
        "    # Average normals for each vertex\n",
        "    normals = []\n",
        "    for i in range(num_points):\n",
        "        n1 = expanded[i - 1]\n",
        "        n2 = expanded[i]\n",
        "        avg_normal = (n1 + n2)\n",
        "        norm = np.linalg.norm(avg_normal)\n",
        "        if norm != 0:\n",
        "            avg_normal = avg_normal / norm\n",
        "        normals.append(avg_normal)\n",
        "\n",
        "    # Offset each point by the average normal * distance\n",
        "    offset_points = points + distance * np.array(normals, dtype=np.float32)\n",
        "\n",
        "    # Return bounding rect of expanded polygon\n",
        "    offset_points_int = np.round(offset_points).astype(np.int32)\n",
        "    return cv2.boundingRect(offset_points_int)\n",
        "\n",
        "def box_score(pred, points, assume_straight_pages = True):\n",
        "    # Compute the confidence score for a polygon : mean of the p values on the polygon\n",
        "    h, w = pred.shape[:2]\n",
        "    xmin = np.clip(np.floor(points[:, 0].min()).astype(np.int32), 0, w - 1)\n",
        "    xmax = np.clip(np.ceil(points[:, 0].max()).astype(np.int32), 0, w - 1)\n",
        "    ymin = np.clip(np.floor(points[:, 1].min()).astype(np.int32), 0, h - 1)\n",
        "    ymax = np.clip(np.ceil(points[:, 1].max()).astype(np.int32), 0, h - 1)\n",
        "    return pred[ymin : ymax + 1, xmin : xmax + 1].mean()\n",
        "\n",
        "def bitmap_to_boxes(pred, bitmap, box_thresh = 0.1, unclip_ratio = 1.7):\n",
        "    # get contours from connected components on the bitmap\n",
        "    contours, _ = cv2.findContours(bitmap.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    boxes = []\n",
        "    for contour in contours:\n",
        "        # Check whether smallest enclosing bounding box is not too small\n",
        "        if np.any(contour[:, 0].max(axis=0) - contour[:, 0].min(axis=0) < 2):\n",
        "            continue\n",
        "        # Compute objectness\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        points: np.ndarray = np.array([[x, y], [x, y + h], [x + w, y + h], [x + w, y]])\n",
        "        score = box_score(pred, points, assume_straight_pages=True)\n",
        "        if score < box_thresh:  # remove polygons with a weak objectness\n",
        "            continue\n",
        "\n",
        "        _box = polygon_to_box(points, unclip_ratio)\n",
        "\n",
        "        # compute relative polygon to get rid of img shape\n",
        "        x, y, w, h = _box\n",
        "        xmin, ymin, xmax, ymax = x, y, x + w, y + h\n",
        "        boxes.append([xmin, ymin, xmax, ymax, score])\n",
        "\n",
        "    return boxes\n",
        "\n",
        "def detector_postprocess(out_map, padding, size, bin_thresh = 0.1, box_thresh = 0.1, unclip_ratio = 1.7):\n",
        "    height, width = size\n",
        "    prob = out_map.squeeze(0).detach().cpu().numpy()\n",
        "    bitmap = (prob >= bin_thresh).astype(np.uint8)\n",
        "    bitmap = cv2.morphologyEx(bitmap, cv2.MORPH_OPEN, np.ones((3, 3), dtype=np.uint8))\n",
        "    H, W = bitmap.shape\n",
        "    _boxes = bitmap_to_boxes(prob, bitmap, box_thresh=box_thresh, unclip_ratio=unclip_ratio)\n",
        "    l, r, t, b = padding\n",
        "    h, w = H - t - b, W - l - r\n",
        "    boxes = []\n",
        "    for _box in _boxes:\n",
        "        objectness = _box[4]\n",
        "        xmin, ymin, xmax, ymax = np.int32(np.round( (_box[:4] - np.array([l, t, l, t])) * np.array([width/w, height/h, width/w, height/h]) ))\n",
        "        box = [ max(xmin-1,0), max(ymin,0), min(xmax+1,width-1), min(ymax+1,height-1), objectness ]\n",
        "        boxes.append(box)\n",
        "    bitmap = cv2.resize(bitmap[t:H-b,l:W-r], (size[1],size[0]))\n",
        "    return boxes, bitmap*255\n",
        "\n",
        "boxes, bitmap = detector_postprocess(prob_map[0], padding, image.shape[:2])"
      ],
      "metadata": {
        "id": "EJvQADmwyzg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(bitmap)"
      ],
      "metadata": {
        "id": "VCF41FX2E2tJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disp = cv2.cvtColor(bitmap,cv2.COLOR_GRAY2BGR)\n",
        "for box in boxes:\n",
        "    cv2.rectangle(disp,(box[0],box[1]),(box[2],box[3]),(0,255,0),2)\n",
        "cv2_imshow(disp)"
      ],
      "metadata": {
        "id": "YIIsSwMfFFjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recognizer (RNN)"
      ],
      "metadata": {
        "id": "EmX0X5R5aUl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "box, confidence = np.int32(boxes[index][:4]), float(boxes[index][4])\n",
        "print(box,confidence)"
      ],
      "metadata": {
        "id": "kKS4KmynaXld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crop = image[box[1]:box[3]+1,box[0]:box[2]+1]\n",
        "print(crop.shape)"
      ],
      "metadata": {
        "id": "DE9NbM59ap7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(crop)"
      ],
      "metadata": {
        "id": "PN6XlbU8bdqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = model.reco_predictor\n",
        "rnn.to(device)"
      ],
      "metadata": {
        "id": "yYbjomCEaI4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recognition preprocesing\n",
        "batch = rnn.pre_processor([crop])[0].to(device)\n",
        "batch.shape, batch.min().item(), batch.max().item()"
      ],
      "metadata": {
        "id": "7Lfojd5aeXpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recognition\n",
        "features = rnn.model.feat_extractor(batch)\n",
        "b, c, h, w = features.shape\n",
        "features_seq = torch.reshape(features, shape=(-1, h * c, w))\n",
        "features_seq = torch.transpose(features_seq, 1, 2)\n",
        "hidden_states, _ = rnn.model.decoder(features_seq)\n",
        "logits = rnn.model.linear(hidden_states)"
      ],
      "metadata": {
        "id": "CIAwlentjRXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn.model.feat_extractor"
      ],
      "metadata": {
        "id": "UZEhzdgsjkcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features.shape"
      ],
      "metadata": {
        "id": "CWRfUtWVjmMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_seq.shape"
      ],
      "metadata": {
        "id": "VYItjCMbkDPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_states.shape"
      ],
      "metadata": {
        "id": "0Hm72cIzkH-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn.model.linear"
      ],
      "metadata": {
        "id": "2hS2egNIkmd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn.model.linear.bias.abs().max().item()"
      ],
      "metadata": {
        "id": "jM7SbrXclEKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "id": "hfCEbveTkrHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits"
      ],
      "metadata": {
        "id": "86hkcJEmkn45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recogniton postprocessing"
      ],
      "metadata": {
        "id": "r7AVwB5Xq8fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidates = logits[0].softmax(dim=-1).argmax(dim=-1)\n",
        "candidates"
      ],
      "metadata": {
        "id": "AN54wAjTnfTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(candidates)"
      ],
      "metadata": {
        "id": "ofGmSMlQp4Mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits[0].softmax(dim=-1).max(dim=-1).values"
      ],
      "metadata": {
        "id": "z1l2fkHMrm23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confidence = logits[0].softmax(dim=-1).max(dim=-1).values.min(dim=-1).values.item()\n",
        "confidence"
      ],
      "metadata": {
        "id": "YssS_S8AqbPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn.model.postprocessor"
      ],
      "metadata": {
        "id": "k-RuxsWvmgfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn.model.postprocessor.vocab"
      ],
      "metadata": {
        "id": "fLnSKWq9orJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(rnn.model.postprocessor.vocab)"
      ],
      "metadata": {
        "id": "FVo2WaCbmx-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = candidates[candidates < len(rnn.model.postprocessor.vocab)].tolist()\n",
        "ids"
      ],
      "metadata": {
        "id": "v5xM2EDBo2gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = ''.join(rnn.model.postprocessor.vocab[i] for i in ids)\n",
        "result"
      ],
      "metadata": {
        "id": "fUNd0yCvqWFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# more sophisticated recogniton postprocessing (CTC)\n",
        "result = rnn.model.postprocessor(logits)"
      ],
      "metadata": {
        "id": "TmPeH0GAl3tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "GsUmk-HdlubX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}